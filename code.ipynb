{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b281c2af",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Product Reviews\n",
    "\n",
    "## Topics\n",
    "\n",
    "1.0 Importing packages and loading data\n",
    "\n",
    "    1.1 About the dataset\n",
    "    \n",
    "    1.2 Loading the dataset\n",
    "\n",
    "    1.3 Exploratory Data Analysis (EDA)\n",
    "\n",
    "2.0 Cleaning text for analysis\n",
    "\n",
    "3.0 Plotting the wordcloud using TfIdf Vectorizer\n",
    "\n",
    "4.0 Running different ML models\n",
    "\n",
    "    4.1 Naive Baye's Classifier\n",
    "    \n",
    "    4.2 Logistic Regression Classifier\n",
    "    \n",
    "    4.3 CNN\n",
    "    \n",
    "    4.4 RNN using LSTM\n",
    "\n",
    "5.0 Interpreting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719189e1",
   "metadata": {},
   "source": [
    "## 1.0 Importing packages and loading data\n",
    "\n",
    "Import all the packages and load the required data from tensorflow datasets of amazon_reviews of watches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e235e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow_datasets as tfds\n",
    "from textblob import TextBlob\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, roc_auc_score\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3778a",
   "metadata": {},
   "source": [
    "### 1.1 About the dataset\n",
    "This is a dataset of amazon reviews of watches collected from amazon with the official rights.\n",
    "\n",
    "A repository of 1.8 million reviews, with relevant features such as customer_id, category, product_id, product_title, review_body, review_headline, star_rating, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95669e",
   "metadata": {},
   "source": [
    "### 1.2 Loading the dataset\n",
    "The downloaded data is in zip file of train and test, and we will be working on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62228027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_watch = tfds.load('amazon_us_reviews/Watches_v1_00', split='train', shuffle_files= True)\n",
    "ds_watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_watch = tfds.as_dataframe(ds_watch.take(10000))\n",
    "df_watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea78957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_watch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9633859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_watch.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_watch_new = df_watch[['data/review_body', 'data/review_headline', 'data/star_rating']]\n",
    "df_watch_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_watch_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516dfa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_watch_new[\"data/star_rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df_watch_new[\"data/star_rating\"].quantile(0.25)\n",
    "q3 = df_watch_new[\"data/star_rating\"].quantile(0.75)\n",
    "q1, q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ae3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = q3-q1\n",
    "iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = q3 +1.5*iqr\n",
    "lower = q1 - 1.5*iqr\n",
    "upper, lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b548f",
   "metadata": {},
   "source": [
    "All the texts are encoded in utf-8 format and needs to be decoded to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375ddbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40389764/how-to-translate-bytes-objects-into-literal-strings-in-pandas-dataframe-pytho\n",
    "df_watch_new['data/review_body'] = df_watch_new['data/review_body'].str.decode(\"utf-8\")\n",
    "df_watch_new['data/review_headline'] = df_watch_new['data/review_headline'].str.decode(\"utf-8\")\n",
    "df_watch_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "df_watch_new[\"polarity_scores\"] = df_watch_new[\"data/review_body\"].apply(analyser.polarity_scores)\n",
    "df_watch_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a26f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_rating(polarity_scores):\n",
    "    if polarity_scores >= -1.0 and polarity_scores < -0.6:\n",
    "        return 1\n",
    "    elif polarity_scores >= -0.6 and polarity_scores < -0.2:\n",
    "        return 2\n",
    "    elif polarity_scores >= -0.2 and polarity_scores < 0.2:\n",
    "        return 3\n",
    "    elif polarity_scores >= 0.2 and polarity_scores < 0.6:\n",
    "        return 4\n",
    "    elif polarity_scores >= 0.6 and polarity_scores <= 1.0:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f678683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_watch_new['extracted_rating'] = df_watch_new['polarity_scores'].apply(lambda x: extracting_rating(x['compound']))\n",
    "df_watch_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554331c",
   "metadata": {},
   "source": [
    "#### We can see below the count of the new categories and the data looks dominating by 5 star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_watch_new.groupby(['extracted_rating']).size())\n",
    "print(df_watch_new.groupby(['data/star_rating']).size())\n",
    "df_watch_new.groupby(['extracted_rating']).size().plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ff068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_watch_new.groupby(['data/star_rating']).size().plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7f19c",
   "metadata": {},
   "source": [
    "As we can see from above that the actual rating of the reviews lokks not to b that correct when compared to the sentiment taht is expressed in the text review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04800ffc",
   "metadata": {},
   "source": [
    "## 2.0 Cleaning text for analysis\n",
    "\n",
    "All the important steps of cleaning the text are performed in the API clean_text.\n",
    "\n",
    "1. Lower the text\n",
    "2. Cleaning the words to remove unwanted characters(important step)\n",
    "3. Stop words removal\n",
    "4. Remove lengthening of words\n",
    "5. Perform Lemmatization\n",
    "6. Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    # Convert words to lower case\n",
    "    return text.lower()\n",
    "    \n",
    "def format_words_remove_unwanted_chars(text):\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', ' ', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    # <br and > have been done separately because they are getting treated as different words rather than a single tag\n",
    "    text = re.sub(r'<br ', ' ', text)\n",
    "    text = re.sub(r'>', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\\\\\\\', ' ', text)\n",
    "    text = re.sub(r'<p><br [\\/]?><[\\/]?p>', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \",text)\n",
    "    return text\n",
    "    \n",
    "def remove_stopwords(text):  \n",
    "    # Removing stop words\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    return text\n",
    "\n",
    "def remove_lengthening(text):\n",
    "    # Remove Lengthening\n",
    "    patt = re.compile(r\"(.)\\1{2,}\")\n",
    "    length_list = [patt.sub(r\"\\1\",word) for word in text]\n",
    "    return length_list\n",
    "    \n",
    "def lemmatizing(text):\n",
    "    # Lemmatizer\n",
    "    lem = WordNetLemmatizer()\n",
    "    lem_list = [lem.lemmatize(word) for word in text]\n",
    "    return lem_list\n",
    "\n",
    "def stemming(text):\n",
    "    # Stemming\n",
    "    porter = PorterStemmer()\n",
    "    stem_list = [porter.stem(word) for word in text]\n",
    "    stem_list = list(dict.fromkeys(stem_list))\n",
    "    return \" \".join(stem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87deac",
   "metadata": {},
   "source": [
    "Below piece of code will perform all the data cleaning operations and give us a cleaned text which is added as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbd89c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_watch_new['clean_review'] = df_watch_new['data/review_body'].apply(lambda x: convert_to_lower(x))\n",
    "df_watch_new['clean_review'] = df_watch_new['clean_review'].apply(lambda x: format_words_remove_unwanted_chars(x))\n",
    "df_watch_new['clean_review'] = df_watch_new['clean_review'].apply(lambda x: remove_stopwords(x))\n",
    "#df_watch_new['clean_review'] = df_watch_new['clean_review'].apply(lambda x: remove_lengthening(x))\n",
    "df_watch_new['clean_review'] = df_watch_new['clean_review'].apply(lambda x: lemmatizing(x))\n",
    "df_watch_new['clean_review'] = df_watch_new['clean_review'].apply(lambda x: stemming(x))\n",
    "df_watch_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(df_watch_new['clean_review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769b3c0",
   "metadata": {},
   "source": [
    "## 3.0 Plotting the wordcloud using TfIdf Vectorizer\n",
    "Making a WordCloud of the vocabulary extracted from the cleaned abstracts using the Tf-Idf vectorizer.\n",
    "#### Tf-Idf Vectorizer:\n",
    "Term frequency-inverse document frequency (tf-idf) gives a measure that takes the importance of a word in consideration depending on how frequently it occurs in a document and a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_watch_new['clean_review'])\n",
    "column_names = vectorizer.get_feature_names()\n",
    "df1 = pd.DataFrame(X.toarray(), columns = column_names, index = df_watch_new['clean_review'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eec01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = X.toarray()\n",
    "docs = X_mat[(X_mat>0).any(axis=1)]\n",
    "words = np.array(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec928bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies={}\n",
    "for i in range(len(docs)):\n",
    "    doc = docs[i] \n",
    "    idx = (doc>0)\n",
    "    doc_words = words[idx]\n",
    "    doc_counts = doc[doc>0]\n",
    "    dict1 = dict(zip(doc_words, doc_counts))\n",
    "    frequencies.update(dict1)\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd306ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white').fit_words(frequencies)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "_ = ax.imshow(wordcloud, interpolation='bilinear')\n",
    "_ = ax.axis(\"off\")\n",
    "fig.savefig(\"tfidfvect_wordcloud.png\", bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc532511",
   "metadata": {},
   "source": [
    "## 4.0 Running different ML models\n",
    "Splitting the dataset into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80 and 20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_watch_new[\"clean_review\"], df_watch_new[\"data/star_rating\"], stratify=df_watch_new[\"data/star_rating\"], random_state=1234, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48e07b",
   "metadata": {},
   "source": [
    "### 4.1 Naive Baye's Classifier\n",
    "A naive Bayes classifier is an algorithm for classifying objects that employs Bayes' theorem. Naive Bayes classifiers are based on the assumption of strong (or naive) independence between data point attributes.\n",
    "\n",
    "We will be running our train and test on Naive Baye's model and measure the accuracy, f1 score, precision, recall, and roc_auc score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88eab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# fit\n",
    "vectorizer.fit(X_train)\n",
    "# transform training data\n",
    "X_train_dtm = vectorizer.transform(X_train)\n",
    "# equivalently: combine fit and transform into a single step\n",
    "# this is faster and what most people would do\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# instantiate a Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "# using X_train_dtm\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# calculate accuracy of class predictions\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46752626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (poorly calibrated)\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)\n",
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e754b3",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression Classifier\n",
    "Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.\n",
    "\n",
    "We will be running our train and test on Logistic Regression model and measure the accuracy, f1 score, precision, recall, and roc_auc score.\n",
    "\n",
    "the performance of the Logistice Regression Classifier is mostly better when comapre dto the Naive Baye's but it is lot slower when compared to it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c659b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "# calculate accuracy of class predictions\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c064ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)\n",
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf48efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(rating):\n",
    "    if rating > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d51adb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_watch_new[[\"binary_rating\"]] = df_watch_new[\"data/star_rating\"].apply(lambda i: binary(i))\n",
    "df_watch_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb370a",
   "metadata": {},
   "source": [
    "### 4.3 CNN\n",
    "\n",
    "\n",
    "We will be now splitting the train dataset into train and validation sets for training on the Neural Network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X_train, y_train, stratify= y_train, random_state=57643892, test_size=0.2)\n",
    "print(train_texts.shape)\n",
    "print(val_texts.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ff304",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4bb4c",
   "metadata": {},
   "source": [
    "Keras includes some tools for converting text to formats that deep learning models can use. I've already done some processing, so all that remains is for me to run a Tokenizer with the top 12000 words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 12000\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "val_texts = tokenizer.texts_to_sequences(val_texts)\n",
    "test_texts = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d94ce",
   "metadata": {},
   "source": [
    "#### Padding Sequences\n",
    "To effectively use batches, I'll need to convert my sequences into sequences of the same length. Everything here will be the length of the longest sentence in the training set. I'm not dealing with it here, but having variable lengths may be advantageous so that each batch contains sentences of similar lengths. This may help to alleviate problems caused by too many padded elements in a sequence. Padding modes are also available, which may be useful for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeeef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
    "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
    "val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)\n",
    "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91212a2a",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Net Model\n",
    "Here, I'm just using some basic models. This CNN has a 64-dimensional embedding, three convolutional layers, the first two of which have max pooling and the last of which has global max pooling. The results are then routed through a dense layer and finally to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [keras.metrics.Precision(name='precision'),\n",
    "           keras.metrics.Recall(name='recall'),\n",
    "           keras.metrics.CategoricalAccuracy(name='categorical_accuracy')]\n",
    "def build_model():\n",
    "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
    "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
    "    x = layers.MaxPool1D(3)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.MaxPool1D(5)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit(train_texts, train_labels, batch_size=128, epochs=2, validation_data=(val_texts, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit(train_texts, train_labels, batch_size=128, epochs=2, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_texts)\n",
    "metric = keras.metrics.CategoricalAccuracy()\n",
    "metric.update_state(y_test,preds)\n",
    "print('Categorical accuracy score: {:0.4}'.format(metric.result().numpy()))\n",
    "metric = keras.metrics.Precision()\n",
    "metric.update_state(y_test,preds)\n",
    "print('Precision: {:0.4}'.format(metric.result().numpy()))\n",
    "metric = keras.metrics.Recall()\n",
    "metric.update_state(y_test,preds)\n",
    "print('Recall: {:0.4}'.format(metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29a547",
   "metadata": {},
   "source": [
    "### 4.4 RNN\n",
    "#### Recurrent Neural Net Model\n",
    "I'm going to use a simple model for an RNN model as well. This is made up of an embedding, two LSTM layers, two dense layers, and the output layer. Because the latter require GPU, I'm using the LSTM rather than the GRU/CuDNNGRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
    "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
    "    x = layers.LSTM(128, return_sequences=True)(embedded)\n",
    "    x = layers.LSTM(128)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "rnn_model = build_rnn_model()\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rnn_model.fit(train_texts, train_labels, batch_size=128, epochs=1, validation_data=(val_texts, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f607e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnn_model.predict(test_texts)\n",
    "metric = keras.metrics.CategoricalAccuracy()\n",
    "metric.update_state(y_test,preds)\n",
    "print('Categorical accuracy score: {:0.4}'.format(metric.result().numpy()))\n",
    "metric = keras.metrics.Precision()\n",
    "metric.update_state(y_test,preds)\n",
    "print('Precision: {:0.4}'.format(metric.result().numpy()))\n",
    "metric = keras.metrics.Recall()\n",
    "metric.update_state(y_test,preds)\n",
    "print('Recall: {:0.4}'.format(metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dd736",
   "metadata": {},
   "source": [
    "## 5.0 Interpreting the results\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c6c22fcc",
   "metadata": {},
   "source": [
    "As we try the deep learning models the results are exceptional but the run time is very high as shown in the table below.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbe1e2",
   "metadata": {},
   "source": [
    "#### What else could we do?\n",
    "I haven't tried a lot of things here. Other fields from the original Amazon data, I believe, could be added to the model. Furthermore, we did not include any global features from the samples, such as length, character level features, and so on. We could even try character-level deep learning models to see if they can reduce sensitivity to misspellings. Character level characteristics may be important in online reviews because users may misspell things to avoid moderation. However, because these models are already performing near-perfectly, any gains will be minimal at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d8eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
